# Use Cases for Team Review - Omnify MVP

**Date**: December 2024  
**Purpose**: 5 detailed use cases for team comparison and selection  
**Goal**: Team will select 8 most important use cases from 5×5=25 total proposals

---

## Use Case 1: From Blindness to Prescriptive Action

**Status**: ✅ Core Use Case (Reference)

The ideal customer profile (ICP) for the initial MVP is a CMO of a mid-market DTC brand (e.g., Sarah, CMO of a $75M brand) who uses platforms like Shopify, Meta, Google, and TripleWhale.

**Persona**: Sarah, CMO, DTC Brand ($75M Revenue, $1.5M/mo Ad Spend).  
**Pain**: "Flying blind, wasting $200k+/month on the wrong channels" due to broken post-iOS14 attribution and conflicting metrics (TripleWhale shows last-click, HubSpot shows first-touch).  
**Goal**: Receive daily, actionable, cross-platform guidance to shift budget and maximize ROAS.

| Step | Module(s) Used | Action/Narrative Flow (CMO Perspective) | Evidence Alignment |
|------|----------------|----------------------------------------|-------------------|
| 1. Unify Data & See Truth | MEMORY (Attribution) | Sarah logs into the FACE dashboard (or the iframe embed within HubSpot/TripleWhale). She sees a single, unified view of performance, instantly clarifying that the Time-Decay Multi-Touch attribution shows Meta is 42% of revenue and Google is 31%. | Solves the Reporting & Analytics Limitations pain (17% average complaint rate) and the Cross-Platform Unified Reporting gap. |
| 2. Identify Risk & Prediction | ORACLE (Prediction) | The FACE dashboard shows a Predictive Alert Panel. ORACLE flags: "Meta ROAS dropped 12% week-over-week due to creative fatigue. Risk factors: High frequency and audience saturation". | ORACLE solves the Attribution Inaccuracies (Weighted Score 4.25) and provides the required 7-day fatigue prediction capability. |
| 3. Prescriptive Action | CURIOSITY (Recommendation) | Sarah reviews the top Insight Card: "Recommendation: Shift $50k from Meta prospecting to Google brand keywords" with an Expected Lift of $75k Revenue. The system provides a clear rationale ("Google Search is driving 3x more profit"). | Directly addresses the No Prescriptive AI/What to Do Next universal gap. The output must be clear (action, reason, expected impact). |
| 4. Execute & Impact | FACE (UI) + MEMORY (Logging) | Sarah clicks an "Apply Recommendation" button. The system logs the decision (in MEMORY) and executes the budget shift (in V1, this means drafting the move for human approval). | Omnify achieves the core value proposition: 18% reduction in wasted ad spend ($270k saved annually) at a price point ($799/mo) that is affordable compared to Northbeam ($2,500–3,500/mo). |

---

## Use Case 2: From Reactive Firefighting to Proactive Prevention

**ICP**: Marketing Operations Manager at a high-growth SaaS company managing multiple channels and campaigns.

**Persona**: Marcus, Marketing Ops Manager, SaaS Company ($120M ARR, $2.5M/mo Ad Spend).  
**Pain**: "We're always reacting to problems after they happen. By the time we notice a campaign is underperforming, we've already wasted $50k. We need to catch issues before they impact revenue."  
**Goal**: Receive proactive alerts and automated recommendations to prevent budget waste before campaigns fail.

| Step | Module(s) Used | Action/Narrative Flow (Ops Manager Perspective) | Evidence Alignment |
|------|----------------|------------------------------------------------|-------------------|
| 1. Early Warning Detection | ORACLE (Prediction) + MEMORY (Historical Data) | Marcus opens the FACE dashboard Monday morning. ORACLE shows a "Pre-Fatigue Alert" for Campaign X: "Creative set 'Summer Sale' shows 15% engagement drop over 3 days. Predicted to hit fatigue threshold in 4 days. Current spend: $12k/day." | Addresses the need for proactive intelligence (vs. reactive reporting). Prevents waste before it happens. |
| 2. Root Cause Analysis | ORACLE (Risk Factors) + MEMORY (Attribution) | The system displays: "Root Cause: Audience overlap (78%) with Campaign Y. Frequency is 3.2x (optimal: 2.0x). Recommendation: Rotate creative or reduce frequency by 30%." | Solves the "Why is this happening?" question that current tools don't answer. |
| 3. Automated Prevention Plan | CURIOSITY (Recommendation) | CURIOSITY generates: "Action Plan: 1) Pause 'Summer Sale' creative set (saves $48k over 4 days). 2) Activate backup creative 'Summer Sale V2' (expected ROAS: 3.2x). 3) Reduce frequency cap to 2.0x. Expected outcome: Maintain revenue while reducing waste by $48k." | Provides not just "what to do" but a complete action plan with expected outcomes. |
| 4. One-Click Execution | FACE (UI) + MEMORY (Logging) | Marcus reviews the plan and clicks "Execute Prevention Plan". The system logs the action, pauses the fatigued creative, activates the backup, and adjusts frequency caps. Marcus receives a confirmation: "Prevention plan executed. Expected savings: $48k. Monitoring active." | Enables fast action without manual intervention across multiple platforms. |

**Success Metric**: Reduce reactive firefighting incidents by 60% and prevent $200k+ in wasted spend per quarter.

---

## Use Case 3: From Fragmented Reporting to Executive Clarity

**ICP**: VP of Marketing at an enterprise B2B company managing complex multi-touch attribution across long sales cycles.

**Persona**: Jennifer, VP Marketing, B2B Enterprise ($500M Revenue, $5M/mo Marketing Spend).  
**Pain**: "I spend 3 hours every Monday reconciling reports from 8 different tools. My CEO asks 'What's working?' and I can't give a clear answer because the numbers don't match. I need one source of truth that shows the full customer journey."  
**Goal**: Present a unified, executive-ready view of marketing performance that tells a clear story from first touch to revenue.

| Step | Module(s) Used | Action/Narrative Flow (VP Marketing Perspective) | Evidence Alignment |
|------|----------------|---------------------------------------------------|-------------------|
| 1. Unified Attribution View | MEMORY (Multi-Touch Attribution) | Jennifer opens the FACE dashboard. Instead of 8 different reports, she sees one unified view: "Q4 Performance: 1,247 leads → 89 opportunities → $12.4M revenue. Top path: LinkedIn (32%) → Content (28%) → Sales Call (40%)." | Solves the Cross-Platform Unified Reporting gap and eliminates manual reconciliation. |
| 2. Executive Summary Generation | MEMORY (Aggregation) + CURIOSITY (Insights) | The dashboard auto-generates an executive summary: "Q4 Highlights: LinkedIn campaigns drove 40% of pipeline. Content marketing ROI improved 25% YoY. Sales cycle shortened by 12% for content-nurtured leads." | Provides executive-ready insights without manual analysis. |
| 3. Channel Performance Clarity | MEMORY (Attribution) + ORACLE (Trends) | Jennifer sees: "Channel Performance: LinkedIn ($2.1M spend, $8.4M revenue, 4.0x ROAS) is outperforming Google Ads ($1.8M spend, $5.2M revenue, 2.9x ROAS). Trend: LinkedIn improving, Google declining." | Clear, actionable channel comparison with trend analysis. |
| 4. Board-Ready Export | FACE (UI) + MEMORY (Data Export) | Jennifer clicks "Export Board Report". The system generates a PDF with unified metrics, channel performance, and recommendations. She sends it to the CEO 5 minutes later. | Eliminates hours of manual report creation. |

**Success Metric**: Reduce reporting time from 3 hours/week to 15 minutes/week. Enable data-driven board presentations.

---

## Use Case 4: From Manual Budget Optimization to AI-Driven Allocation

**ICP**: Performance Marketing Director at a fast-growing e-commerce brand managing dynamic budget allocation across multiple channels.

**Persona**: David, Performance Marketing Director, E-commerce ($200M Revenue, $3M/mo Ad Spend).  
**Pain**: "I manually adjust budgets every day based on performance, but by the time I make changes, opportunities are lost. I need the system to automatically reallocate budget to winning channels in real-time."  
**Goal**: Automate budget reallocation based on real-time performance signals, maximizing ROAS without manual intervention.

| Step | Module(s) Used | Action/Narrative Flow (Performance Director Perspective) | Evidence Alignment |
|------|----------------|----------------------------------------------------------|-------------------|
| 1. Real-Time Performance Monitoring | MEMORY (Real-Time Data) + ORACLE (Performance Prediction) | David sets up automated budget rules: "If Meta ROAS > 4.0x and Google ROAS < 2.5x, shift 20% of Google budget to Meta." The system monitors performance every hour. | Enables real-time optimization vs. daily manual adjustments. |
| 2. Opportunity Detection | ORACLE (Trend Analysis) | At 2 PM, ORACLE detects: "Meta campaign 'Holiday Sale' ROAS increased from 3.8x to 4.5x in last 4 hours. Google campaign 'Brand Search' ROAS dropped from 3.2x to 2.3x. Opportunity: Shift $15k from Google to Meta." | Identifies optimization opportunities faster than human analysis. |
| 3. Automated Recommendation | CURIOSITY (Budget Recommendation) | CURIOSITY generates: "Budget Shift Recommendation: Move $15k from Google 'Brand Search' to Meta 'Holiday Sale'. Expected impact: +$22.5k revenue (1.5x ROAS improvement). Risk: Low (both campaigns stable)." | Provides clear rationale and expected impact for automated decisions. |
| 4. Automated Execution (with Approval) | FACE (UI) + MEMORY (Logging) | David has enabled "Auto-Execute High-Confidence Shifts" (>$10k expected lift, low risk). The system executes the shift and sends a notification: "Budget shift executed: $15k Google → Meta. Expected lift: $22.5k. Monitoring for 24h." | Balances automation with control. Logs all decisions for audit. |

**Success Metric**: Increase ROAS by 15% through automated budget optimization. Reduce manual budget management time by 80%.

---

## Use Case 5: From Channel Silos to Cross-Channel Orchestration

**ICP**: Growth Marketing Manager at a subscription-based DTC brand managing complex customer journeys across multiple touchpoints.

**Persona**: Lisa, Growth Marketing Manager, Subscription DTC ($50M Revenue, $800k/mo Ad Spend).  
**Pain**: "Each channel team (Meta, Google, Email) optimizes their own metrics, but we're not coordinating. Customers see the same ad 5 times across channels, wasting budget. We need to orchestrate the entire customer journey."  
**Goal**: Coordinate campaigns across channels to eliminate waste, reduce ad fatigue, and optimize the full customer journey.

| Step | Module(s) Used | Action/Narrative Flow (Growth Manager Perspective) | Evidence Alignment |
|------|----------------|-----------------------------------------------------|-------------------|
| 1. Cross-Channel Journey Mapping | MEMORY (Multi-Touch Attribution) + ORACLE (Journey Analysis) | Lisa opens the "Customer Journey" view. The system shows: "Typical path: Meta Ad (Day 1) → Email (Day 3) → Google Search (Day 5) → Purchase (Day 7). Current issue: 45% of customers see Meta ad 3+ times before email, causing fatigue." | Identifies cross-channel coordination opportunities. |
| 2. Fatigue Detection Across Channels | ORACLE (Cross-Channel Fatigue) | ORACLE flags: "Customer segment 'High-Intent' shows 3.5x frequency across Meta + Email. Fatigue risk: High. Recommendation: Suppress Meta ads for customers who received email in last 24h." | Prevents cross-channel ad fatigue that single-channel tools miss. |
| 3. Orchestration Recommendation | CURIOSITY (Cross-Channel Strategy) | CURIOSITY recommends: "Orchestration Plan: 1) Suppress Meta ads for email-engaged users (saves $12k/month). 2) Increase Google Search budget for email-clickers (expected +$18k revenue). 3) Coordinate Meta retargeting to run only after email opens. Expected outcome: +$18k revenue, -$12k waste." | Provides cross-channel strategy, not just single-channel optimization. |
| 4. Automated Orchestration | FACE (UI) + MEMORY (Logging) | Lisa enables "Cross-Channel Orchestration Mode". The system automatically suppresses Meta ads for email-engaged users, increases Google Search bids for email-clickers, and coordinates timing. She receives: "Orchestration active. Monitoring 1,247 customers across 3 channels." | Enables true cross-channel coordination that's impossible with siloed tools. |

**Success Metric**: Reduce cross-channel ad frequency by 40%. Increase conversion rate by 25% through better journey coordination. Save $150k/month in wasted spend.

---

## Use Case Comparison Matrix

| Use Case | Primary Pain Point | Key Module | Complexity | Expected Impact | Implementation Effort |
|----------|-------------------|------------|------------|-----------------|---------------------|
| **1. Blindness → Prescriptive** | Attribution confusion, no clear actions | CURIOSITY | Medium | High ($270k saved/year) | Medium |
| **2. Reactive → Proactive** | Firefighting, late detection | ORACLE | High | Very High ($200k+ prevented/quarter) | High |
| **3. Fragmented → Executive Clarity** | Manual reporting, no single source of truth | MEMORY | Low | Medium (3h → 15min/week) | Low |
| **4. Manual → AI Budget Allocation** | Daily manual optimization | CURIOSITY + ORACLE | High | High (15% ROAS increase) | High |
| **5. Silos → Cross-Channel** | Channel coordination, waste | MEMORY + ORACLE | Very High | Very High ($150k/month saved) | Very High |

---

## Selection Criteria for Team Review

When selecting the 8 most important use cases from all proposals, consider:

1. **Business Impact**: Revenue saved, ROAS improved, time saved
2. **Technical Feasibility**: Can we build this in MVP timeframe?
3. **Market Demand**: How many customers have this pain?
4. **Competitive Differentiation**: Does this set us apart?
5. **User Experience**: Is the journey clear and exciting?
6. **Implementation Complexity**: Can we deliver quality in time?

---

## Next Steps

1. **Team Review**: Each team member reviews all 5 use cases
2. **Individual Proposals**: Each team member proposes 5 additional use cases (following same format)
3. **Consolidation**: Collect all proposals (5 × 5 = 25 total)
4. **Prioritization**: Team votes/ranks all 25 use cases
5. **Selection**: Select top 8 use cases for MVP
6. **Roadmap**: Map selected use cases to development sprints

---

**Status**: Ready for team review and additional use case proposals

